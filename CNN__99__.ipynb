{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaJaAd4ndY4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "\n",
        "class KddData(object):\n",
        "\n",
        "    def __init__(self, batch_size):\n",
        "        kddcup99 = datasets.fetch_kddcup99()\n",
        "        self._encoder = {\n",
        "            'protocal': LabelEncoder(),\n",
        "            'service':  LabelEncoder(),\n",
        "            'flag':     LabelEncoder(),\n",
        "            'label':    LabelEncoder()\n",
        "        }\n",
        "        self.batch_size = batch_size\n",
        "        data_X, data_y = self.__encode_data(kddcup99.data, kddcup99.target)\n",
        "        self.train_dataset, self.test_dataset = self.__split_data_to_tensor(data_X, data_y)\n",
        "        self.train_dataloader = DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n",
        "        self.test_dataloader = DataLoader(self.test_dataset, self.batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    \n",
        "    def __encode_data(self, data_X, data_y):\n",
        "        self._encoder['protocal'].fit(list(set(data_X[:, 1])))\n",
        "        self._encoder['service'].fit(list(set(data_X[:, 2])))\n",
        "        self._encoder['flag'].fit((list(set(data_X[:, 3]))))\n",
        "        self._encoder['label'].fit(list(set(data_y)))\n",
        "        data_X[:, 1] = self._encoder['protocal'].transform(data_X[:, 1])\n",
        "        data_X[:, 2] = self._encoder['service'].transform(data_X[:, 2])\n",
        "        data_X[:, 3] = self._encoder['flag'].transform(data_X[:, 3])\n",
        "        data_X = np.pad(data_X, ((0, 0), (0, 64 - len(data_X[0]))), 'constant').reshape(-1, 1, 8, 8)\n",
        "        data_y = self._encoder['label'].transform(data_y)\n",
        "        return data_X, data_y\n",
        "\n",
        "    \n",
        "    def __split_data_to_tensor(self, data_X, data_y):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.3)\n",
        "        train_dataset = TensorDataset(\n",
        "            torch.from_numpy(X_train.astype(np.float32)),\n",
        "            torch.from_numpy(y_train.astype(np.int))\n",
        "        )\n",
        "        test_dataset = TensorDataset(\n",
        "            torch.from_numpy(X_test.astype(np.float32)),\n",
        "            torch.from_numpy(y_test.astype(np.int))\n",
        "        )\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "    \n",
        "    def decode(self, data, label=False):\n",
        "        if not label:\n",
        "            _data = list(data)\n",
        "            _data[1] = self._encoder['protocal'].inverse_transform([_data[1]])[0]\n",
        "            _data[2] = self._encoder['service'].inverse_transform([_data[2]])[0]\n",
        "            _data[2] = self._encoder['flag'].inverse_transform([_data[3]])[0]\n",
        "            return _data\n",
        "        return self._encoder['label'].inverse_transform(data)\n",
        "    \n",
        "    def encode(self, data, label=False):\n",
        "        if not label:\n",
        "            _data = list(data)\n",
        "            _data[1] = self._encoder['protocal'].transform([_data[1]])[0]\n",
        "            _data[2] = self._encoder['service'].transform([_data[2]])[0]\n",
        "            _data[3] = self._encoder['flag'].transform([_data[3]])[0]\n",
        "            return _data\n",
        "        return self._encoder['label'].transform([data])[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgkNU7TpfUMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_dim, n_class):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, 6, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(6, 16, 3, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(144, 512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Linear(256, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX5L537DfZ37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b08452c2-43d9-4894-ff6f-2891a148641b"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "batch_size = 128\n",
        "learning_rate = 1e-2\n",
        "num_epoches = 20\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "dataset = KddData(batch_size)\n",
        "model = CNN(1, 23)\n",
        "\n",
        "def train():\n",
        "    \n",
        "    global model\n",
        "\n",
        "    if USE_GPU:\n",
        "        model = model.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epoches):\n",
        "        print('epoch {}'.format(epoch + 1))\n",
        "        print('*' * 10)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        for i, data in enumerate(dataset.train_dataloader, 1):\n",
        "            img, label = data\n",
        "            if USE_GPU:\n",
        "                img = img.cuda()\n",
        "                label = label.cuda()\n",
        "            img = Variable(img)\n",
        "            label = Variable(label)\n",
        "            \n",
        "            out = model(img)\n",
        "            loss = criterion(out, label)\n",
        "            running_loss += loss.item() * label.size(0)\n",
        "            _, pred = torch.max(out, 1)\n",
        "            num_correct = (pred == label).sum()\n",
        "            accuracy = (pred == label).float().mean()\n",
        "            running_acc += num_correct.item()\n",
        "           \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print('Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(\n",
        "            epoch + 1, running_loss / (len(dataset.train_dataset)), running_acc / (len(\n",
        "                dataset.train_dataset))))\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        eval_acc = 0\n",
        "        for data in dataset.test_dataloader:\n",
        "            img, label = data\n",
        "            if USE_GPU:\n",
        "                img = Variable(img, volatile=True).cuda()\n",
        "                label = Variable(label, volatile=True).cuda()\n",
        "            else:\n",
        "                img = Variable(img, volatile=True)\n",
        "                label = Variable(label, volatile=True)\n",
        "            out = model(img)\n",
        "            loss = criterion(out, label)\n",
        "            eval_loss += loss.item() * label.size(0)\n",
        "            _, pred = torch.max(out, 1)\n",
        "            num_correct = (pred == label).sum()\n",
        "            eval_acc += num_correct.item()\n",
        "        print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
        "            dataset.test_dataset)), eval_acc / (len(dataset.test_dataset))))\n",
        "        print()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://ndownloader.figshare.com/files/5976042\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLjoMOjufkZ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e93327be-8f08-4366-92c1-7fc4f3e0fe93"
      },
      "source": [
        "train()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "**********\n",
            "Finish 1 epoch, Loss: 0.173646, Acc: 0.965395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 4.329107, Acc: 0.197912\n",
            "\n",
            "epoch 2\n",
            "**********\n",
            "Finish 2 epoch, Loss: 0.121380, Acc: 0.975718\n",
            "Test Loss: 0.071552, Acc: 0.984960\n",
            "\n",
            "epoch 3\n",
            "**********\n",
            "Finish 3 epoch, Loss: 0.061168, Acc: 0.985972\n",
            "Test Loss: 0.051967, Acc: 0.988894\n",
            "\n",
            "epoch 4\n",
            "**********\n",
            "Finish 4 epoch, Loss: 0.048974, Acc: 0.988109\n",
            "Test Loss: 0.044623, Acc: 0.989218\n",
            "\n",
            "epoch 5\n",
            "**********\n",
            "Finish 5 epoch, Loss: 0.043078, Acc: 0.988404\n",
            "Test Loss: 0.039736, Acc: 0.988779\n",
            "\n",
            "epoch 6\n",
            "**********\n",
            "Finish 6 epoch, Loss: 0.039922, Acc: 0.989009\n",
            "Test Loss: 0.038127, Acc: 0.989596\n",
            "\n",
            "epoch 7\n",
            "**********\n",
            "Finish 7 epoch, Loss: 0.037329, Acc: 0.989616\n",
            "Test Loss: 0.034903, Acc: 0.990122\n",
            "\n",
            "epoch 8\n",
            "**********\n",
            "Finish 8 epoch, Loss: 0.035285, Acc: 0.989905\n",
            "Test Loss: 0.033525, Acc: 0.990318\n",
            "\n",
            "epoch 9\n",
            "**********\n",
            "Finish 9 epoch, Loss: 0.033958, Acc: 0.990165\n",
            "Test Loss: 0.032454, Acc: 0.990075\n",
            "\n",
            "epoch 10\n",
            "**********\n",
            "Finish 10 epoch, Loss: 0.032801, Acc: 0.990301\n",
            "Test Loss: 0.031588, Acc: 0.990311\n",
            "\n",
            "epoch 11\n",
            "**********\n",
            "Finish 11 epoch, Loss: 0.031667, Acc: 0.990457\n",
            "Test Loss: 0.030104, Acc: 0.990871\n",
            "\n",
            "epoch 12\n",
            "**********\n",
            "Finish 12 epoch, Loss: 0.030827, Acc: 0.990882\n",
            "Test Loss: 0.029568, Acc: 0.991026\n",
            "\n",
            "epoch 13\n",
            "**********\n",
            "Finish 13 epoch, Loss: 0.030045, Acc: 0.991105\n",
            "Test Loss: 0.028913, Acc: 0.990810\n",
            "\n",
            "epoch 14\n",
            "**********\n",
            "Finish 14 epoch, Loss: 0.029296, Acc: 0.991380\n",
            "Test Loss: 0.028379, Acc: 0.992376\n",
            "\n",
            "epoch 15\n",
            "**********\n",
            "Finish 15 epoch, Loss: 0.028703, Acc: 0.991675\n",
            "Test Loss: 0.028998, Acc: 0.991424\n",
            "\n",
            "epoch 16\n",
            "**********\n",
            "Finish 16 epoch, Loss: 0.027928, Acc: 0.991941\n",
            "Test Loss: 0.027226, Acc: 0.991640\n",
            "\n",
            "epoch 17\n",
            "**********\n",
            "Finish 17 epoch, Loss: 0.027450, Acc: 0.992247\n",
            "Test Loss: 0.027107, Acc: 0.992551\n",
            "\n",
            "epoch 18\n",
            "**********\n",
            "Finish 18 epoch, Loss: 0.026851, Acc: 0.992455\n",
            "Test Loss: 0.025798, Acc: 0.994049\n",
            "\n",
            "epoch 19\n",
            "**********\n",
            "Finish 19 epoch, Loss: 0.026395, Acc: 0.992745\n",
            "Test Loss: 0.025141, Acc: 0.993637\n",
            "\n",
            "epoch 20\n",
            "**********\n",
            "Finish 20 epoch, Loss: 0.025772, Acc: 0.993060\n",
            "Test Loss: 0.025850, Acc: 0.993340\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}